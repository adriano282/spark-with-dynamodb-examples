<?xml version="1.0" encoding="UTF-8"?>
<testsuite name="Testing the stock shares peak price by month" tests="1" skipped="0" failures="0" errors="0" timestamp="2020-04-26T00:53:11" hostname="adriano-Inspiron-5570" time="7.701">
  <properties/>
  <testcase name="Successfully run the job" classname="Testing the stock shares peak price by month" time="7.701"/>
  <system-out><![CDATA[]]></system-out>
  <system-err><![CDATA[Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
20/04/25 21:53:12 INFO SparkContext: Running Spark version 2.2.0
20/04/25 21:53:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/04/25 21:53:12 WARN Utils: Your hostname, adriano-Inspiron-5570 resolves to a loopback address: 127.0.1.1; using 192.168.0.101 instead (on interface wlp3s0)
20/04/25 21:53:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
20/04/25 21:53:12 INFO SparkContext: Submitted application: job
20/04/25 21:53:12 INFO SecurityManager: Changing view acls to: adriano
20/04/25 21:53:12 INFO SecurityManager: Changing modify acls to: adriano
20/04/25 21:53:12 INFO SecurityManager: Changing view acls groups to: 
20/04/25 21:53:12 INFO SecurityManager: Changing modify acls groups to: 
20/04/25 21:53:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(adriano); groups with view permissions: Set(); users  with modify permissions: Set(adriano); groups with modify permissions: Set()
20/04/25 21:53:12 INFO Utils: Successfully started service 'sparkDriver' on port 33089.
20/04/25 21:53:12 INFO SparkEnv: Registering MapOutputTracker
20/04/25 21:53:12 INFO SparkEnv: Registering BlockManagerMaster
20/04/25 21:53:12 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/04/25 21:53:12 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/04/25 21:53:12 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-67964a6b-bd07-48d9-ac4e-3fb95c58f50c
20/04/25 21:53:12 INFO MemoryStore: MemoryStore started with capacity 870.9 MB
20/04/25 21:53:12 INFO SparkEnv: Registering OutputCommitCoordinator
20/04/25 21:53:12 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/04/25 21:53:13 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.101:4040
20/04/25 21:53:13 INFO Executor: Starting executor ID driver on host localhost
20/04/25 21:53:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44607.
20/04/25 21:53:13 INFO NettyBlockTransferService: Server created on 192.168.0.101:44607
20/04/25 21:53:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/04/25 21:53:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.101, 44607, None)
20/04/25 21:53:13 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.101:44607 with 870.9 MB RAM, BlockManagerId(driver, 192.168.0.101, 44607, None)
20/04/25 21:53:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.101, 44607, None)
20/04/25 21:53:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.101, 44607, None)
20/04/25 21:53:13 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/adriano/Development/study/spark/spark-with-dynamo-examples/spark-warehouse/').
20/04/25 21:53:13 INFO SharedState: Warehouse path is 'file:/home/adriano/Development/study/spark/spark-with-dynamo-examples/spark-warehouse/'.
20/04/25 21:53:13 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/04/25 21:53:13 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 220.0 KB, free 870.7 MB)
20/04/25 21:53:13 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.9 KB, free 870.7 MB)
20/04/25 21:53:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.101:44607 (size: 20.9 KB, free: 870.9 MB)
20/04/25 21:53:13 INFO SparkContext: Created broadcast 0 from hadoopRDD at Main.java:193
20/04/25 21:53:14 INFO SparkSqlParser: Parsing command: NYSE
ANTLR Tool version 4.5.3 used for code generation does not match the current runtime version 4.7.2ANTLR Runtime version 4.5.3 used for parser compilation does not match the current runtime version 4.7.2ANTLR Tool version 4.5.3 used for code generation does not match the current runtime version 4.7.2ANTLR Runtime version 4.5.3 used for parser compilation does not match the current runtime version 4.7.220/04/25 21:53:14 INFO SparkSqlParser: Parsing command: select substring(date, 1, 7) as month, stockSymbol as stockShareSymbol, max(stockPriceHigh) as peakPrice from nyse group by substring(date, 1, 7), stockSymbol
20/04/25 21:53:15 INFO CodeGenerator: Code generated in 143.465785 ms
20/04/25 21:53:15 INFO CodeGenerator: Code generated in 18.153144 ms
20/04/25 21:53:15 INFO ContextCleaner: Cleaned accumulator 0
20/04/25 21:53:15 INFO deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
20/04/25 21:53:15 INFO JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=
20/04/25 21:53:15 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:15 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.read.AbstractDynamoDBInputFormat.getSplits(AbstractDynamoDBInputFormat.java:47)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:194)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
	at org.apache.spark.ShuffleDependency.<init>(Dependency.scala:91)
	at org.apache.spark.sql.execution.exchange.ShuffleExchange$.prepareShuffleDependency(ShuffleExchange.scala:264)
	at org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:87)
	at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:124)
	at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
	at org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)
	at org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1.apply(SortAggregateExec.scala:77)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1.apply(SortAggregateExec.scala:75)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec.doExecute(SortAggregateExec.scala:75)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
	at org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:95)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)
	at org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:2581)
	at org.apache.spark.sql.Dataset.rdd(Dataset.scala:2578)
	at org.apache.spark.sql.Dataset.toJavaRDD(Dataset.scala:2591)
	at org.apache.spark.sql.Dataset.javaRDD(Dataset.scala:2598)
	at br.com.adriano.swde.Main.main(Main.java:227)
	at br.com.adriano.swde.stepdefs.SharesHighestPriceMonthlyStepDefs.runSharesHighestPriceMonthly(SharesHighestPriceMonthlyStepDefs.java:61)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at io.cucumber.java.Invoker.invoke(Invoker.java:27)
	at io.cucumber.java.JavaStepDefinition.execute(JavaStepDefinition.java:27)
	at io.cucumber.core.runner.PickleStepDefinitionMatch.runStep(PickleStepDefinitionMatch.java:63)
	at io.cucumber.core.runner.TestStep.executeStep(TestStep.java:64)
	at io.cucumber.core.runner.TestStep.run(TestStep.java:49)
	at io.cucumber.core.runner.PickleStepTestStep.run(PickleStepTestStep.java:46)
	at io.cucumber.core.runner.TestCase.run(TestCase.java:51)
	at io.cucumber.core.runner.Runner.runPickle(Runner.java:66)
	at io.cucumber.junit.PickleRunners$NoStepDescriptions.run(PickleRunners.java:149)
	at io.cucumber.junit.FeatureRunner.runChild(FeatureRunner.java:83)
	at io.cucumber.junit.FeatureRunner.runChild(FeatureRunner.java:24)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at io.cucumber.junit.Cucumber.runChild(Cucumber.java:185)
	at io.cucumber.junit.Cucumber.runChild(Cucumber.java:83)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at io.cucumber.junit.Cucumber$RunCucumber.evaluate(Cucumber.java:219)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecuter.runTestClass(JUnitTestClassExecuter.java:114)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecuter.execute(JUnitTestClassExecuter.java:57)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassProcessor.processTestClass(JUnitTestClassProcessor.java:66)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32)
	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93)
	at com.sun.proxy.$Proxy2.processTestClass(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:109)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:147)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:129)
	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:404)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63)
	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:46)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:15 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:15 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:15 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:15 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:15 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:15 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:15 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:15 INFO AbstractDynamoDBInputFormat: Read percentage: 0.5
20/04/25 21:53:15 INFO AbstractDynamoDBInputFormat: Would use 0 segments for size
20/04/25 21:53:15 INFO AbstractDynamoDBInputFormat: Would use 0 segments for throughput
20/04/25 21:53:15 INFO AbstractDynamoDBInputFormat: Using computed number of segments: 1
20/04/25 21:53:15 INFO AbstractDynamoDBInputFormat: Max number of cluster map tasks: 5
20/04/25 21:53:15 INFO AbstractDynamoDBInputFormat: Configured read throughput: 1
20/04/25 21:53:15 INFO AbstractDynamoDBInputFormat: Calculated to use 1 mappers
20/04/25 21:53:15 INFO AbstractDynamoDBInputFormat: Using 1 segments across 1 mappers
20/04/25 21:53:15 INFO DynamoDBSplitGenerator: Generating 1 segments for 1 max mappers
20/04/25 21:53:15 INFO DynamoDBSplitGenerator: Assigning 1 segments to mapper 0: [0]
20/04/25 21:53:15 WARN FileOutputCommitter: Output Path is null in setupJob()
20/04/25 21:53:15 INFO SparkContext: Starting job: saveAsHadoopDataset at Main.java:250
20/04/25 21:53:15 INFO DAGScheduler: Registering RDD 7 (javaRDD at Main.java:227)
20/04/25 21:53:15 INFO DAGScheduler: Got job 0 (saveAsHadoopDataset at Main.java:250) with 200 output partitions
20/04/25 21:53:15 INFO DAGScheduler: Final stage: ResultStage 1 (saveAsHadoopDataset at Main.java:250)
20/04/25 21:53:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/04/25 21:53:15 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/04/25 21:53:15 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[7] at javaRDD at Main.java:227), which has no missing parents
20/04/25 21:53:15 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 17.8 KB, free 870.6 MB)
20/04/25 21:53:15 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.6 KB, free 870.6 MB)
20/04/25 21:53:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.101:44607 (size: 8.6 KB, free: 870.9 MB)
20/04/25 21:53:15 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
20/04/25 21:53:15 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[7] at javaRDD at Main.java:227) (first 15 tasks are for partitions Vector(0))
20/04/25 21:53:15 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
20/04/25 21:53:15 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4855 bytes)
20/04/25 21:53:15 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
20/04/25 21:53:15 INFO HadoopRDD: Input split: org.apache.hadoop.dynamodb.split.DynamoDBSegmentsSplit@621629a7
20/04/25 21:53:15 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:15 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:15 INFO ReadIopsCalculator: Table name: StockShare
20/04/25 21:53:15 INFO ReadIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:15 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockSymbol,AttributeType: S}, {AttributeName: date,AttributeType: S}],TableName: StockShare,KeySchema: [{AttributeName: stockSymbol,KeyType: HASH}, {AttributeName: date,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 2885,ItemCount: 19,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/StockShare,}}
20/04/25 21:53:15 INFO ReadIopsCalculator: Throughput per task for table StockShare : 1
20/04/25 21:53:15 INFO RateController: Rate controller initialized. target rate=1.0, bucket capacity=5.0
20/04/25 21:53:15 INFO AbstractDynamoDBRecordReader: Total segments: 1
20/04/25 21:53:15 INFO AbstractDynamoDBRecordReader: Segment count of this mapper: 1
20/04/25 21:53:15 INFO AbstractDynamoDBRecordReader: Segments of this mapper: [0]
20/04/25 21:53:15 INFO AbstractDynamoDBRecordReader: Approximate item count of this mapper: 0
20/04/25 21:53:15 INFO CodeGenerator: Code generated in 16.980451 ms
20/04/25 21:53:15 INFO PageResultMultiplexer: Added a page. Page count: 1
20/04/25 21:53:15 INFO AbstractReadManager: Segment 0 complete. Remaining segments: 0
20/04/25 21:53:15 INFO AbstractReadManager: Shutting down record reader, no segments remaining.
20/04/25 21:53:15 INFO ReadWorker: Worker shutting down, no longer alive
20/04/25 21:53:15 INFO CodeGenerator: Code generated in 14.465675 ms
20/04/25 21:53:15 INFO CodeGenerator: Code generated in 8.752302 ms
20/04/25 21:53:15 INFO PageResultMultiplexer: Pagemux stats: items=0, pages=1, cap=600
20/04/25 21:53:15 INFO AbstractDynamoDBRecordReader: Closing down record reader
20/04/25 21:53:15 INFO AbstractReadManager: Shutting down record reader, no segments remaining.
20/04/25 21:53:15 INFO CodeGenerator: Code generated in 5.762269 ms
20/04/25 21:53:15 INFO CodeGenerator: Code generated in 5.974305 ms
20/04/25 21:53:15 INFO CodeGenerator: Code generated in 6.582332 ms
20/04/25 21:53:15 INFO CodeGenerator: Code generated in 6.502606 ms
20/04/25 21:53:15 INFO CodeGenerator: Code generated in 5.501405 ms
20/04/25 21:53:15 INFO CodeGenerator: Code generated in 10.05189 ms
20/04/25 21:53:15 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2014 bytes result sent to driver
20/04/25 21:53:15 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 296 ms on localhost (executor driver) (1/1)
20/04/25 21:53:15 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/04/25 21:53:15 INFO DAGScheduler: ShuffleMapStage 0 (javaRDD at Main.java:227) finished in 0,308 s
20/04/25 21:53:15 INFO DAGScheduler: looking for newly runnable stages
20/04/25 21:53:15 INFO DAGScheduler: running: Set()
20/04/25 21:53:15 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/04/25 21:53:15 INFO DAGScheduler: failed: Set()
20/04/25 21:53:15 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[13] at mapToPair at Main.java:228), which has no missing parents
20/04/25 21:53:15 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 83.3 KB, free 870.6 MB)
20/04/25 21:53:15 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.8 KB, free 870.5 MB)
20/04/25 21:53:15 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.101:44607 (size: 32.8 KB, free: 870.8 MB)
20/04/25 21:53:15 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
20/04/25 21:53:15 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at mapToPair at Main.java:228) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/04/25 21:53:15 INFO TaskSchedulerImpl: Adding task set 1.0 with 200 tasks
20/04/25 21:53:15 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:15 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
20/04/25 21:53:15 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
20/04/25 21:53:16 INFO CodeGenerator: Code generated in 6.284378 ms
20/04/25 21:53:16 INFO CodeGenerator: Code generated in 6.891723 ms
20/04/25 21:53:16 INFO CodeGenerator: Code generated in 5.999637 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000000_1
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000000_1
20/04/25 21:53:16 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2267 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 88 ms on localhost (executor driver) (1/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000001_2
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000001_2
20/04/25 21:53:16 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 29 ms on localhost (executor driver) (2/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000002_3
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000002_3
20/04/25 21:53:16 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 26 ms on localhost (executor driver) (3/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.101:44607 in memory (size: 8.6 KB, free: 870.8 MB)
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000003_4
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000003_4
20/04/25 21:53:16 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2267 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5, localhost, executor driver, partition 4, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 38 ms on localhost (executor driver) (4/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000004_5
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000004_5
20/04/25 21:53:16 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6, localhost, executor driver, partition 5, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 24 ms on localhost (executor driver) (5/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000005_6
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000005_6
20/04/25 21:53:16 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7, localhost, executor driver, partition 6, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 21 ms on localhost (executor driver) (6/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000006_7
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000006_7
20/04/25 21:53:16 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8, localhost, executor driver, partition 7, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 7.0 in stage 1.0 (TID 8)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 29 ms on localhost (executor driver) (7/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000007_8
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000007_8
20/04/25 21:53:16 INFO Executor: Finished task 7.0 in stage 1.0 (TID 8). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 9, localhost, executor driver, partition 8, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 8.0 in stage 1.0 (TID 9)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 20 ms on localhost (executor driver) (8/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000008_9
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000008_9
20/04/25 21:53:16 INFO Executor: Finished task 8.0 in stage 1.0 (TID 9). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 10, localhost, executor driver, partition 9, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 9.0 in stage 1.0 (TID 10)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 9) in 25 ms on localhost (executor driver) (9/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000009_10
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000009_10
20/04/25 21:53:16 INFO Executor: Finished task 9.0 in stage 1.0 (TID 10). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 11, localhost, executor driver, partition 10, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 10) in 22 ms on localhost (executor driver) (10/200)
20/04/25 21:53:16 INFO Executor: Running task 10.0 in stage 1.0 (TID 11)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000010_11
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000010_11
20/04/25 21:53:16 INFO Executor: Finished task 10.0 in stage 1.0 (TID 11). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 12, localhost, executor driver, partition 11, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 11.0 in stage 1.0 (TID 12)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 11) in 25 ms on localhost (executor driver) (11/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000011_12
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000011_12
20/04/25 21:53:16 INFO Executor: Finished task 11.0 in stage 1.0 (TID 12). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 13, localhost, executor driver, partition 12, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 12.0 in stage 1.0 (TID 13)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 12) in 25 ms on localhost (executor driver) (12/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000012_13
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000012_13
20/04/25 21:53:16 INFO Executor: Finished task 12.0 in stage 1.0 (TID 13). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 14, localhost, executor driver, partition 13, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 13.0 in stage 1.0 (TID 14)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 13) in 29 ms on localhost (executor driver) (13/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000013_14
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000013_14
20/04/25 21:53:16 INFO Executor: Finished task 13.0 in stage 1.0 (TID 14). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 15, localhost, executor driver, partition 14, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 14.0 in stage 1.0 (TID 15)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 14) in 28 ms on localhost (executor driver) (14/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000014_15
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000014_15
20/04/25 21:53:16 INFO Executor: Finished task 14.0 in stage 1.0 (TID 15). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 16, localhost, executor driver, partition 15, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 15.0 in stage 1.0 (TID 16)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 15) in 32 ms on localhost (executor driver) (15/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000015_16
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000015_16
20/04/25 21:53:16 INFO Executor: Finished task 15.0 in stage 1.0 (TID 16). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 17, localhost, executor driver, partition 16, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 16.0 in stage 1.0 (TID 17)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 16) in 26 ms on localhost (executor driver) (16/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000016_17
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000016_17
20/04/25 21:53:16 INFO Executor: Finished task 16.0 in stage 1.0 (TID 17). 2267 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 18, localhost, executor driver, partition 17, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 17.0 in stage 1.0 (TID 18)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 17) in 43 ms on localhost (executor driver) (17/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000017_18
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000017_18
20/04/25 21:53:16 INFO Executor: Finished task 17.0 in stage 1.0 (TID 18). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 19, localhost, executor driver, partition 18, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 18.0 in stage 1.0 (TID 19)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 18) in 20 ms on localhost (executor driver) (18/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000018_19
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000018_19
20/04/25 21:53:16 INFO Executor: Finished task 18.0 in stage 1.0 (TID 19). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 20, localhost, executor driver, partition 19, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 19.0 in stage 1.0 (TID 20)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 19) in 27 ms on localhost (executor driver) (19/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000019_20
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000019_20
20/04/25 21:53:16 INFO Executor: Finished task 19.0 in stage 1.0 (TID 20). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 21, localhost, executor driver, partition 20, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 20.0 in stage 1.0 (TID 21)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 20) in 19 ms on localhost (executor driver) (20/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000020_21
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000020_21
20/04/25 21:53:16 INFO Executor: Finished task 20.0 in stage 1.0 (TID 21). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 22, localhost, executor driver, partition 21, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 21.0 in stage 1.0 (TID 22)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 21) in 18 ms on localhost (executor driver) (21/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000021_22
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000021_22
20/04/25 21:53:16 INFO Executor: Finished task 21.0 in stage 1.0 (TID 22). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 23, localhost, executor driver, partition 22, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 22) in 21 ms on localhost (executor driver) (22/200)
20/04/25 21:53:16 INFO Executor: Running task 22.0 in stage 1.0 (TID 23)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000022_23
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000022_23
20/04/25 21:53:16 INFO Executor: Finished task 22.0 in stage 1.0 (TID 23). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 23.0 in stage 1.0 (TID 24, localhost, executor driver, partition 23, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 23.0 in stage 1.0 (TID 24)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 23) in 20 ms on localhost (executor driver) (23/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000023_24
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000023_24
20/04/25 21:53:16 INFO Executor: Finished task 23.0 in stage 1.0 (TID 24). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 24.0 in stage 1.0 (TID 25, localhost, executor driver, partition 24, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 23.0 in stage 1.0 (TID 24) in 19 ms on localhost (executor driver) (24/200)
20/04/25 21:53:16 INFO Executor: Running task 24.0 in stage 1.0 (TID 25)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000024_25
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000024_25
20/04/25 21:53:16 INFO Executor: Finished task 24.0 in stage 1.0 (TID 25). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 25.0 in stage 1.0 (TID 26, localhost, executor driver, partition 25, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 25.0 in stage 1.0 (TID 26)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 24.0 in stage 1.0 (TID 25) in 19 ms on localhost (executor driver) (25/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000025_26
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000025_26
20/04/25 21:53:16 INFO Executor: Finished task 25.0 in stage 1.0 (TID 26). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 26.0 in stage 1.0 (TID 27, localhost, executor driver, partition 26, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 26.0 in stage 1.0 (TID 27)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 25.0 in stage 1.0 (TID 26) in 22 ms on localhost (executor driver) (26/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000026_27
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000026_27
20/04/25 21:53:16 INFO Executor: Finished task 26.0 in stage 1.0 (TID 27). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 27.0 in stage 1.0 (TID 28, localhost, executor driver, partition 27, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 26.0 in stage 1.0 (TID 27) in 22 ms on localhost (executor driver) (27/200)
20/04/25 21:53:16 INFO Executor: Running task 27.0 in stage 1.0 (TID 28)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000027_28
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000027_28
20/04/25 21:53:16 INFO Executor: Finished task 27.0 in stage 1.0 (TID 28). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 28.0 in stage 1.0 (TID 29, localhost, executor driver, partition 28, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 27.0 in stage 1.0 (TID 28) in 23 ms on localhost (executor driver) (28/200)
20/04/25 21:53:16 INFO Executor: Running task 28.0 in stage 1.0 (TID 29)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000028_29
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000028_29
20/04/25 21:53:16 INFO Executor: Finished task 28.0 in stage 1.0 (TID 29). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 29.0 in stage 1.0 (TID 30, localhost, executor driver, partition 29, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 29.0 in stage 1.0 (TID 30)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 28.0 in stage 1.0 (TID 29) in 21 ms on localhost (executor driver) (29/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000029_30
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000029_30
20/04/25 21:53:16 INFO Executor: Finished task 29.0 in stage 1.0 (TID 30). 2267 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 30.0 in stage 1.0 (TID 31, localhost, executor driver, partition 30, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 30.0 in stage 1.0 (TID 31)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 29.0 in stage 1.0 (TID 30) in 39 ms on localhost (executor driver) (30/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000030_31
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000030_31
20/04/25 21:53:16 INFO Executor: Finished task 30.0 in stage 1.0 (TID 31). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 31.0 in stage 1.0 (TID 32, localhost, executor driver, partition 31, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 31.0 in stage 1.0 (TID 32)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 30.0 in stage 1.0 (TID 31) in 22 ms on localhost (executor driver) (31/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000031_32
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000031_32
20/04/25 21:53:16 INFO Executor: Finished task 31.0 in stage 1.0 (TID 32). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 32.0 in stage 1.0 (TID 33, localhost, executor driver, partition 32, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 32.0 in stage 1.0 (TID 33)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 31.0 in stage 1.0 (TID 32) in 21 ms on localhost (executor driver) (32/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000032_33
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000032_33
20/04/25 21:53:16 INFO Executor: Finished task 32.0 in stage 1.0 (TID 33). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 33.0 in stage 1.0 (TID 34, localhost, executor driver, partition 33, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 33.0 in stage 1.0 (TID 34)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 32.0 in stage 1.0 (TID 33) in 21 ms on localhost (executor driver) (33/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000033_34
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000033_34
20/04/25 21:53:16 INFO Executor: Finished task 33.0 in stage 1.0 (TID 34). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 34.0 in stage 1.0 (TID 35, localhost, executor driver, partition 34, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 33.0 in stage 1.0 (TID 34) in 18 ms on localhost (executor driver) (34/200)
20/04/25 21:53:16 INFO Executor: Running task 34.0 in stage 1.0 (TID 35)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000034_35
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000034_35
20/04/25 21:53:16 INFO Executor: Finished task 34.0 in stage 1.0 (TID 35). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 35.0 in stage 1.0 (TID 36, localhost, executor driver, partition 35, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 34.0 in stage 1.0 (TID 35) in 18 ms on localhost (executor driver) (35/200)
20/04/25 21:53:16 INFO Executor: Running task 35.0 in stage 1.0 (TID 36)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000035_36
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000035_36
20/04/25 21:53:16 INFO Executor: Finished task 35.0 in stage 1.0 (TID 36). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 36.0 in stage 1.0 (TID 37, localhost, executor driver, partition 36, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 36.0 in stage 1.0 (TID 37)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 35.0 in stage 1.0 (TID 36) in 17 ms on localhost (executor driver) (36/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000036_37
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000036_37
20/04/25 21:53:16 INFO Executor: Finished task 36.0 in stage 1.0 (TID 37). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 37.0 in stage 1.0 (TID 38, localhost, executor driver, partition 37, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 37.0 in stage 1.0 (TID 38)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 36.0 in stage 1.0 (TID 37) in 22 ms on localhost (executor driver) (37/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000037_38
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000037_38
20/04/25 21:53:16 INFO Executor: Finished task 37.0 in stage 1.0 (TID 38). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 38.0 in stage 1.0 (TID 39, localhost, executor driver, partition 38, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 38.0 in stage 1.0 (TID 39)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 37.0 in stage 1.0 (TID 38) in 17 ms on localhost (executor driver) (38/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000038_39
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000038_39
20/04/25 21:53:16 INFO Executor: Finished task 38.0 in stage 1.0 (TID 39). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 39.0 in stage 1.0 (TID 40, localhost, executor driver, partition 39, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 39.0 in stage 1.0 (TID 40)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 38.0 in stage 1.0 (TID 39) in 18 ms on localhost (executor driver) (39/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000039_40
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000039_40
20/04/25 21:53:16 INFO Executor: Finished task 39.0 in stage 1.0 (TID 40). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 40.0 in stage 1.0 (TID 41, localhost, executor driver, partition 40, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 40.0 in stage 1.0 (TID 41)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 39.0 in stage 1.0 (TID 40) in 16 ms on localhost (executor driver) (40/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000040_41
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000040_41
20/04/25 21:53:16 INFO Executor: Finished task 40.0 in stage 1.0 (TID 41). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 41.0 in stage 1.0 (TID 42, localhost, executor driver, partition 41, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 41.0 in stage 1.0 (TID 42)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 40.0 in stage 1.0 (TID 41) in 19 ms on localhost (executor driver) (41/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000041_42
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000041_42
20/04/25 21:53:16 INFO Executor: Finished task 41.0 in stage 1.0 (TID 42). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 42.0 in stage 1.0 (TID 43, localhost, executor driver, partition 42, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO Executor: Running task 42.0 in stage 1.0 (TID 43)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 41.0 in stage 1.0 (TID 42) in 15 ms on localhost (executor driver) (42/200)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000042_43
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:16 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:16 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:16 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000042_43
20/04/25 21:53:16 INFO Executor: Finished task 42.0 in stage 1.0 (TID 43). 2224 bytes result sent to driver
20/04/25 21:53:16 INFO TaskSetManager: Starting task 43.0 in stage 1.0 (TID 44, localhost, executor driver, partition 43, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:16 INFO TaskSetManager: Finished task 42.0 in stage 1.0 (TID 43) in 17 ms on localhost (executor driver) (43/200)
20/04/25 21:53:16 INFO Executor: Running task 43.0 in stage 1.0 (TID 44)
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/04/25 21:53:16 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:16 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:16 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:16 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:16 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000043_44
20/04/25 21:53:16 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:16 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:16 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:16 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:16 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:16 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:16 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:16 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:16 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:16 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000043_44
20/04/25 21:53:17 INFO Executor: Finished task 43.0 in stage 1.0 (TID 44). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 44.0 in stage 1.0 (TID 45, localhost, executor driver, partition 44, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO Executor: Running task 44.0 in stage 1.0 (TID 45)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 43.0 in stage 1.0 (TID 44) in 21 ms on localhost (executor driver) (44/200)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000044_45
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000044_45
20/04/25 21:53:17 INFO Executor: Finished task 44.0 in stage 1.0 (TID 45). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 45.0 in stage 1.0 (TID 46, localhost, executor driver, partition 45, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO Executor: Running task 45.0 in stage 1.0 (TID 46)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 44.0 in stage 1.0 (TID 45) in 17 ms on localhost (executor driver) (45/200)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000045_46
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000045_46
20/04/25 21:53:17 INFO Executor: Finished task 45.0 in stage 1.0 (TID 46). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 46.0 in stage 1.0 (TID 47, localhost, executor driver, partition 46, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 45.0 in stage 1.0 (TID 46) in 23 ms on localhost (executor driver) (46/200)
20/04/25 21:53:17 INFO Executor: Running task 46.0 in stage 1.0 (TID 47)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000046_47
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000046_47
20/04/25 21:53:17 INFO Executor: Finished task 46.0 in stage 1.0 (TID 47). 2267 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 47.0 in stage 1.0 (TID 48, localhost, executor driver, partition 47, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 46.0 in stage 1.0 (TID 47) in 34 ms on localhost (executor driver) (47/200)
20/04/25 21:53:17 INFO Executor: Running task 47.0 in stage 1.0 (TID 48)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000047_48
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000047_48
20/04/25 21:53:17 INFO Executor: Finished task 47.0 in stage 1.0 (TID 48). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 48.0 in stage 1.0 (TID 49, localhost, executor driver, partition 48, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO Executor: Running task 48.0 in stage 1.0 (TID 49)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 47.0 in stage 1.0 (TID 48) in 19 ms on localhost (executor driver) (48/200)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000048_49
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000048_49
20/04/25 21:53:17 INFO Executor: Finished task 48.0 in stage 1.0 (TID 49). 2267 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 49.0 in stage 1.0 (TID 50, localhost, executor driver, partition 49, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 48.0 in stage 1.0 (TID 49) in 18 ms on localhost (executor driver) (49/200)
20/04/25 21:53:17 INFO Executor: Running task 49.0 in stage 1.0 (TID 50)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000049_50
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000049_50
20/04/25 21:53:17 INFO Executor: Finished task 49.0 in stage 1.0 (TID 50). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 50.0 in stage 1.0 (TID 51, localhost, executor driver, partition 50, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 49.0 in stage 1.0 (TID 50) in 16 ms on localhost (executor driver) (50/200)
20/04/25 21:53:17 INFO Executor: Running task 50.0 in stage 1.0 (TID 51)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000050_51
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000050_51
20/04/25 21:53:17 INFO Executor: Finished task 50.0 in stage 1.0 (TID 51). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 51.0 in stage 1.0 (TID 52, localhost, executor driver, partition 51, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO Executor: Running task 51.0 in stage 1.0 (TID 52)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 50.0 in stage 1.0 (TID 51) in 21 ms on localhost (executor driver) (51/200)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000051_52
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000051_52
20/04/25 21:53:17 INFO Executor: Finished task 51.0 in stage 1.0 (TID 52). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 52.0 in stage 1.0 (TID 53, localhost, executor driver, partition 52, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO Executor: Running task 52.0 in stage 1.0 (TID 53)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 51.0 in stage 1.0 (TID 52) in 19 ms on localhost (executor driver) (52/200)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000052_53
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000052_53
20/04/25 21:53:17 INFO Executor: Finished task 52.0 in stage 1.0 (TID 53). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 53.0 in stage 1.0 (TID 54, localhost, executor driver, partition 53, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO Executor: Running task 53.0 in stage 1.0 (TID 54)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 52.0 in stage 1.0 (TID 53) in 18 ms on localhost (executor driver) (53/200)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000053_54
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000053_54
20/04/25 21:53:17 INFO Executor: Finished task 53.0 in stage 1.0 (TID 54). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 54.0 in stage 1.0 (TID 55, localhost, executor driver, partition 54, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO Executor: Running task 54.0 in stage 1.0 (TID 55)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 53.0 in stage 1.0 (TID 54) in 16 ms on localhost (executor driver) (54/200)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000054_55
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000054_55
20/04/25 21:53:17 INFO Executor: Finished task 54.0 in stage 1.0 (TID 55). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 55.0 in stage 1.0 (TID 56, localhost, executor driver, partition 55, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 54.0 in stage 1.0 (TID 55) in 19 ms on localhost (executor driver) (55/200)
20/04/25 21:53:17 INFO Executor: Running task 55.0 in stage 1.0 (TID 56)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000055_56
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000055_56
20/04/25 21:53:17 INFO Executor: Finished task 55.0 in stage 1.0 (TID 56). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 56.0 in stage 1.0 (TID 57, localhost, executor driver, partition 56, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO Executor: Running task 56.0 in stage 1.0 (TID 57)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 55.0 in stage 1.0 (TID 56) in 18 ms on localhost (executor driver) (56/200)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000056_57
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000056_57
20/04/25 21:53:17 INFO Executor: Finished task 56.0 in stage 1.0 (TID 57). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 57.0 in stage 1.0 (TID 58, localhost, executor driver, partition 57, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO Executor: Running task 57.0 in stage 1.0 (TID 58)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 56.0 in stage 1.0 (TID 57) in 19 ms on localhost (executor driver) (57/200)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000057_58
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000057_58
20/04/25 21:53:17 INFO Executor: Finished task 57.0 in stage 1.0 (TID 58). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 58.0 in stage 1.0 (TID 59, localhost, executor driver, partition 58, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO Executor: Running task 58.0 in stage 1.0 (TID 59)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 57.0 in stage 1.0 (TID 58) in 19 ms on localhost (executor driver) (58/200)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000058_59
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000058_59
20/04/25 21:53:17 INFO Executor: Finished task 58.0 in stage 1.0 (TID 59). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 59.0 in stage 1.0 (TID 60, localhost, executor driver, partition 59, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO Executor: Running task 59.0 in stage 1.0 (TID 60)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 58.0 in stage 1.0 (TID 59) in 17 ms on localhost (executor driver) (59/200)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000059_60
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000059_60
20/04/25 21:53:17 INFO Executor: Finished task 59.0 in stage 1.0 (TID 60). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 60.0 in stage 1.0 (TID 61, localhost, executor driver, partition 60, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 59.0 in stage 1.0 (TID 60) in 21 ms on localhost (executor driver) (60/200)
20/04/25 21:53:17 INFO Executor: Running task 60.0 in stage 1.0 (TID 61)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000060_61
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000060_61
20/04/25 21:53:17 INFO Executor: Finished task 60.0 in stage 1.0 (TID 61). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 61.0 in stage 1.0 (TID 62, localhost, executor driver, partition 61, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 60.0 in stage 1.0 (TID 61) in 17 ms on localhost (executor driver) (61/200)
20/04/25 21:53:17 INFO Executor: Running task 61.0 in stage 1.0 (TID 62)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000061_62
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000061_62
20/04/25 21:53:17 INFO Executor: Finished task 61.0 in stage 1.0 (TID 62). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 62.0 in stage 1.0 (TID 63, localhost, executor driver, partition 62, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 61.0 in stage 1.0 (TID 62) in 17 ms on localhost (executor driver) (62/200)
20/04/25 21:53:17 INFO Executor: Running task 62.0 in stage 1.0 (TID 63)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000062_63
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000062_63
20/04/25 21:53:17 INFO Executor: Finished task 62.0 in stage 1.0 (TID 63). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 63.0 in stage 1.0 (TID 64, localhost, executor driver, partition 63, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 62.0 in stage 1.0 (TID 63) in 23 ms on localhost (executor driver) (63/200)
20/04/25 21:53:17 INFO Executor: Running task 63.0 in stage 1.0 (TID 64)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000063_64
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000063_64
20/04/25 21:53:17 INFO Executor: Finished task 63.0 in stage 1.0 (TID 64). 2267 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 64.0 in stage 1.0 (TID 65, localhost, executor driver, partition 64, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO Executor: Running task 64.0 in stage 1.0 (TID 65)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 63.0 in stage 1.0 (TID 64) in 39 ms on localhost (executor driver) (64/200)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000064_65
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000064_65
20/04/25 21:53:17 INFO Executor: Finished task 64.0 in stage 1.0 (TID 65). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 65.0 in stage 1.0 (TID 66, localhost, executor driver, partition 65, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 64.0 in stage 1.0 (TID 65) in 18 ms on localhost (executor driver) (65/200)
20/04/25 21:53:17 INFO Executor: Running task 65.0 in stage 1.0 (TID 66)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000065_66
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000065_66
20/04/25 21:53:17 INFO Executor: Finished task 65.0 in stage 1.0 (TID 66). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 66.0 in stage 1.0 (TID 67, localhost, executor driver, partition 66, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 65.0 in stage 1.0 (TID 66) in 20 ms on localhost (executor driver) (66/200)
20/04/25 21:53:17 INFO Executor: Running task 66.0 in stage 1.0 (TID 67)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000066_67
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000066_67
20/04/25 21:53:17 INFO Executor: Finished task 66.0 in stage 1.0 (TID 67). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 68.0 in stage 1.0 (TID 68, localhost, executor driver, partition 68, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO Executor: Running task 68.0 in stage 1.0 (TID 68)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 66.0 in stage 1.0 (TID 67) in 20 ms on localhost (executor driver) (67/200)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000068_68
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000068_68
20/04/25 21:53:17 INFO Executor: Finished task 68.0 in stage 1.0 (TID 68). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 69.0 in stage 1.0 (TID 69, localhost, executor driver, partition 69, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO Executor: Running task 69.0 in stage 1.0 (TID 69)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 68.0 in stage 1.0 (TID 68) in 19 ms on localhost (executor driver) (68/200)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000069_69
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000069_69
20/04/25 21:53:17 INFO Executor: Finished task 69.0 in stage 1.0 (TID 69). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 70.0 in stage 1.0 (TID 70, localhost, executor driver, partition 70, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO Executor: Running task 70.0 in stage 1.0 (TID 70)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 69.0 in stage 1.0 (TID 69) in 20 ms on localhost (executor driver) (69/200)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000070_70
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000070_70
20/04/25 21:53:17 INFO Executor: Finished task 70.0 in stage 1.0 (TID 70). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 71.0 in stage 1.0 (TID 71, localhost, executor driver, partition 71, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 70.0 in stage 1.0 (TID 70) in 18 ms on localhost (executor driver) (70/200)
20/04/25 21:53:17 INFO Executor: Running task 71.0 in stage 1.0 (TID 71)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000071_71
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000071_71
20/04/25 21:53:17 INFO Executor: Finished task 71.0 in stage 1.0 (TID 71). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 72.0 in stage 1.0 (TID 72, localhost, executor driver, partition 72, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO Executor: Running task 72.0 in stage 1.0 (TID 72)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 71.0 in stage 1.0 (TID 71) in 16 ms on localhost (executor driver) (71/200)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000072_72
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000072_72
20/04/25 21:53:17 INFO Executor: Finished task 72.0 in stage 1.0 (TID 72). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 73.0 in stage 1.0 (TID 73, localhost, executor driver, partition 73, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 72.0 in stage 1.0 (TID 72) in 16 ms on localhost (executor driver) (72/200)
20/04/25 21:53:17 INFO Executor: Running task 73.0 in stage 1.0 (TID 73)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000073_73
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000073_73
20/04/25 21:53:17 INFO Executor: Finished task 73.0 in stage 1.0 (TID 73). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 74.0 in stage 1.0 (TID 74, localhost, executor driver, partition 74, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 73.0 in stage 1.0 (TID 73) in 16 ms on localhost (executor driver) (73/200)
20/04/25 21:53:17 INFO Executor: Running task 74.0 in stage 1.0 (TID 74)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000074_74
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000074_74
20/04/25 21:53:17 INFO Executor: Finished task 74.0 in stage 1.0 (TID 74). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 75.0 in stage 1.0 (TID 75, localhost, executor driver, partition 75, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO Executor: Running task 75.0 in stage 1.0 (TID 75)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 74.0 in stage 1.0 (TID 74) in 18 ms on localhost (executor driver) (74/200)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000075_75
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000075_75
20/04/25 21:53:17 INFO Executor: Finished task 75.0 in stage 1.0 (TID 75). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 76.0 in stage 1.0 (TID 76, localhost, executor driver, partition 76, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 75.0 in stage 1.0 (TID 75) in 18 ms on localhost (executor driver) (75/200)
20/04/25 21:53:17 INFO Executor: Running task 76.0 in stage 1.0 (TID 76)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000076_76
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000076_76
20/04/25 21:53:17 INFO Executor: Finished task 76.0 in stage 1.0 (TID 76). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 77.0 in stage 1.0 (TID 77, localhost, executor driver, partition 77, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 76.0 in stage 1.0 (TID 76) in 19 ms on localhost (executor driver) (76/200)
20/04/25 21:53:17 INFO Executor: Running task 77.0 in stage 1.0 (TID 77)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000077_77
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000077_77
20/04/25 21:53:17 INFO Executor: Finished task 77.0 in stage 1.0 (TID 77). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 78.0 in stage 1.0 (TID 78, localhost, executor driver, partition 78, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO Executor: Running task 78.0 in stage 1.0 (TID 78)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 77.0 in stage 1.0 (TID 77) in 19 ms on localhost (executor driver) (77/200)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000078_78
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000078_78
20/04/25 21:53:17 INFO Executor: Finished task 78.0 in stage 1.0 (TID 78). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 79.0 in stage 1.0 (TID 79, localhost, executor driver, partition 79, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO Executor: Running task 79.0 in stage 1.0 (TID 79)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 78.0 in stage 1.0 (TID 78) in 19 ms on localhost (executor driver) (78/200)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000079_79
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000079_79
20/04/25 21:53:17 INFO Executor: Finished task 79.0 in stage 1.0 (TID 79). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 80.0 in stage 1.0 (TID 80, localhost, executor driver, partition 80, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 79.0 in stage 1.0 (TID 79) in 16 ms on localhost (executor driver) (79/200)
20/04/25 21:53:17 INFO Executor: Running task 80.0 in stage 1.0 (TID 80)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000080_80
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000080_80
20/04/25 21:53:17 INFO Executor: Finished task 80.0 in stage 1.0 (TID 80). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 81.0 in stage 1.0 (TID 81, localhost, executor driver, partition 81, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 80.0 in stage 1.0 (TID 80) in 16 ms on localhost (executor driver) (80/200)
20/04/25 21:53:17 INFO Executor: Running task 81.0 in stage 1.0 (TID 81)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000081_81
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000081_81
20/04/25 21:53:17 INFO Executor: Finished task 81.0 in stage 1.0 (TID 81). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 82.0 in stage 1.0 (TID 82, localhost, executor driver, partition 82, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 81.0 in stage 1.0 (TID 81) in 19 ms on localhost (executor driver) (81/200)
20/04/25 21:53:17 INFO Executor: Running task 82.0 in stage 1.0 (TID 82)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000082_82
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000082_82
20/04/25 21:53:17 INFO Executor: Finished task 82.0 in stage 1.0 (TID 82). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 83.0 in stage 1.0 (TID 83, localhost, executor driver, partition 83, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 82.0 in stage 1.0 (TID 82) in 18 ms on localhost (executor driver) (82/200)
20/04/25 21:53:17 INFO Executor: Running task 83.0 in stage 1.0 (TID 83)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000083_83
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000083_83
20/04/25 21:53:17 INFO Executor: Finished task 83.0 in stage 1.0 (TID 83). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 84.0 in stage 1.0 (TID 84, localhost, executor driver, partition 84, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 83.0 in stage 1.0 (TID 83) in 27 ms on localhost (executor driver) (83/200)
20/04/25 21:53:17 INFO Executor: Running task 84.0 in stage 1.0 (TID 84)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000084_84
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000084_84
20/04/25 21:53:17 INFO Executor: Finished task 84.0 in stage 1.0 (TID 84). 2267 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 85.0 in stage 1.0 (TID 85, localhost, executor driver, partition 85, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 84.0 in stage 1.0 (TID 84) in 29 ms on localhost (executor driver) (84/200)
20/04/25 21:53:17 INFO Executor: Running task 85.0 in stage 1.0 (TID 85)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000085_85
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000085_85
20/04/25 21:53:17 INFO Executor: Finished task 85.0 in stage 1.0 (TID 85). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 86.0 in stage 1.0 (TID 86, localhost, executor driver, partition 86, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO Executor: Running task 86.0 in stage 1.0 (TID 86)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 85.0 in stage 1.0 (TID 85) in 15 ms on localhost (executor driver) (85/200)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000086_86
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000086_86
20/04/25 21:53:17 INFO Executor: Finished task 86.0 in stage 1.0 (TID 86). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 87.0 in stage 1.0 (TID 87, localhost, executor driver, partition 87, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO Executor: Running task 87.0 in stage 1.0 (TID 87)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 86.0 in stage 1.0 (TID 86) in 13 ms on localhost (executor driver) (86/200)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000087_87
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000087_87
20/04/25 21:53:17 INFO Executor: Finished task 87.0 in stage 1.0 (TID 87). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 88.0 in stage 1.0 (TID 88, localhost, executor driver, partition 88, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO Executor: Running task 88.0 in stage 1.0 (TID 88)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 87.0 in stage 1.0 (TID 87) in 15 ms on localhost (executor driver) (87/200)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000088_88
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000088_88
20/04/25 21:53:17 INFO Executor: Finished task 88.0 in stage 1.0 (TID 88). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 89.0 in stage 1.0 (TID 89, localhost, executor driver, partition 89, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO Executor: Running task 89.0 in stage 1.0 (TID 89)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 88.0 in stage 1.0 (TID 88) in 12 ms on localhost (executor driver) (88/200)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000089_89
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000089_89
20/04/25 21:53:17 INFO Executor: Finished task 89.0 in stage 1.0 (TID 89). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 90.0 in stage 1.0 (TID 90, localhost, executor driver, partition 90, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 89.0 in stage 1.0 (TID 89) in 16 ms on localhost (executor driver) (89/200)
20/04/25 21:53:17 INFO Executor: Running task 90.0 in stage 1.0 (TID 90)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000090_90
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000090_90
20/04/25 21:53:17 INFO Executor: Finished task 90.0 in stage 1.0 (TID 90). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 91.0 in stage 1.0 (TID 91, localhost, executor driver, partition 91, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 90.0 in stage 1.0 (TID 90) in 15 ms on localhost (executor driver) (90/200)
20/04/25 21:53:17 INFO Executor: Running task 91.0 in stage 1.0 (TID 91)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000091_91
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000091_91
20/04/25 21:53:17 INFO Executor: Finished task 91.0 in stage 1.0 (TID 91). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 92.0 in stage 1.0 (TID 92, localhost, executor driver, partition 92, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 91.0 in stage 1.0 (TID 91) in 16 ms on localhost (executor driver) (91/200)
20/04/25 21:53:17 INFO Executor: Running task 92.0 in stage 1.0 (TID 92)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000092_92
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000092_92
20/04/25 21:53:17 INFO Executor: Finished task 92.0 in stage 1.0 (TID 92). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 93.0 in stage 1.0 (TID 93, localhost, executor driver, partition 93, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO Executor: Running task 93.0 in stage 1.0 (TID 93)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 92.0 in stage 1.0 (TID 92) in 19 ms on localhost (executor driver) (92/200)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000093_93
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000093_93
20/04/25 21:53:17 INFO Executor: Finished task 93.0 in stage 1.0 (TID 93). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 94.0 in stage 1.0 (TID 94, localhost, executor driver, partition 94, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO Executor: Running task 94.0 in stage 1.0 (TID 94)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 93.0 in stage 1.0 (TID 93) in 15 ms on localhost (executor driver) (93/200)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000094_94
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000094_94
20/04/25 21:53:17 INFO Executor: Finished task 94.0 in stage 1.0 (TID 94). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 95.0 in stage 1.0 (TID 95, localhost, executor driver, partition 95, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO Executor: Running task 95.0 in stage 1.0 (TID 95)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 94.0 in stage 1.0 (TID 94) in 15 ms on localhost (executor driver) (94/200)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000095_95
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000095_95
20/04/25 21:53:17 INFO Executor: Finished task 95.0 in stage 1.0 (TID 95). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 96.0 in stage 1.0 (TID 96, localhost, executor driver, partition 96, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO Executor: Running task 96.0 in stage 1.0 (TID 96)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 95.0 in stage 1.0 (TID 95) in 18 ms on localhost (executor driver) (95/200)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000096_96
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000096_96
20/04/25 21:53:17 INFO Executor: Finished task 96.0 in stage 1.0 (TID 96). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 97.0 in stage 1.0 (TID 97, localhost, executor driver, partition 97, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 96.0 in stage 1.0 (TID 96) in 18 ms on localhost (executor driver) (96/200)
20/04/25 21:53:17 INFO Executor: Running task 97.0 in stage 1.0 (TID 97)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000097_97
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000097_97
20/04/25 21:53:17 INFO Executor: Finished task 97.0 in stage 1.0 (TID 97). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 98.0 in stage 1.0 (TID 98, localhost, executor driver, partition 98, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO Executor: Running task 98.0 in stage 1.0 (TID 98)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 97.0 in stage 1.0 (TID 97) in 18 ms on localhost (executor driver) (97/200)
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:17 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:17 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:17 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:17 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000098_98
20/04/25 21:53:17 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:17 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:17 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:17 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:17 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:17 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:17 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:17 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:17 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:17 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:17 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:17 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:17 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:17 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000098_98
20/04/25 21:53:17 INFO Executor: Finished task 98.0 in stage 1.0 (TID 98). 2224 bytes result sent to driver
20/04/25 21:53:17 INFO TaskSetManager: Starting task 99.0 in stage 1.0 (TID 99, localhost, executor driver, partition 99, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:17 INFO TaskSetManager: Finished task 98.0 in stage 1.0 (TID 98) in 16 ms on localhost (executor driver) (98/200)
20/04/25 21:53:17 INFO Executor: Running task 99.0 in stage 1.0 (TID 99)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000099_99
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000099_99
20/04/25 21:53:18 INFO Executor: Finished task 99.0 in stage 1.0 (TID 99). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 100.0 in stage 1.0 (TID 100, localhost, executor driver, partition 100, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 99.0 in stage 1.0 (TID 99) in 16 ms on localhost (executor driver) (99/200)
20/04/25 21:53:18 INFO Executor: Running task 100.0 in stage 1.0 (TID 100)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000100_100
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000100_100
20/04/25 21:53:18 INFO Executor: Finished task 100.0 in stage 1.0 (TID 100). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 101.0 in stage 1.0 (TID 101, localhost, executor driver, partition 101, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 101.0 in stage 1.0 (TID 101)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 100.0 in stage 1.0 (TID 100) in 13 ms on localhost (executor driver) (100/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000101_101
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000101_101
20/04/25 21:53:18 INFO Executor: Finished task 101.0 in stage 1.0 (TID 101). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 102.0 in stage 1.0 (TID 102, localhost, executor driver, partition 102, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 102.0 in stage 1.0 (TID 102)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 101.0 in stage 1.0 (TID 101) in 14 ms on localhost (executor driver) (101/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000102_102
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000102_102
20/04/25 21:53:18 INFO Executor: Finished task 102.0 in stage 1.0 (TID 102). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 103.0 in stage 1.0 (TID 103, localhost, executor driver, partition 103, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 103.0 in stage 1.0 (TID 103)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 102.0 in stage 1.0 (TID 102) in 16 ms on localhost (executor driver) (102/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000103_103
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000103_103
20/04/25 21:53:18 INFO Executor: Finished task 103.0 in stage 1.0 (TID 103). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 104.0 in stage 1.0 (TID 104, localhost, executor driver, partition 104, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 104.0 in stage 1.0 (TID 104)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 103.0 in stage 1.0 (TID 103) in 13 ms on localhost (executor driver) (103/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000104_104
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000104_104
20/04/25 21:53:18 INFO Executor: Finished task 104.0 in stage 1.0 (TID 104). 2267 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 105.0 in stage 1.0 (TID 105, localhost, executor driver, partition 105, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 105.0 in stage 1.0 (TID 105)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 104.0 in stage 1.0 (TID 104) in 25 ms on localhost (executor driver) (104/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000105_105
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000105_105
20/04/25 21:53:18 INFO Executor: Finished task 105.0 in stage 1.0 (TID 105). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 106.0 in stage 1.0 (TID 106, localhost, executor driver, partition 106, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 106.0 in stage 1.0 (TID 106)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 105.0 in stage 1.0 (TID 105) in 13 ms on localhost (executor driver) (105/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000106_106
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000106_106
20/04/25 21:53:18 INFO Executor: Finished task 106.0 in stage 1.0 (TID 106). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 107.0 in stage 1.0 (TID 107, localhost, executor driver, partition 107, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 106.0 in stage 1.0 (TID 106) in 14 ms on localhost (executor driver) (106/200)
20/04/25 21:53:18 INFO Executor: Running task 107.0 in stage 1.0 (TID 107)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000107_107
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000107_107
20/04/25 21:53:18 INFO Executor: Finished task 107.0 in stage 1.0 (TID 107). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 108.0 in stage 1.0 (TID 108, localhost, executor driver, partition 108, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 108.0 in stage 1.0 (TID 108)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 107.0 in stage 1.0 (TID 107) in 17 ms on localhost (executor driver) (107/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000108_108
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000108_108
20/04/25 21:53:18 INFO Executor: Finished task 108.0 in stage 1.0 (TID 108). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 109.0 in stage 1.0 (TID 109, localhost, executor driver, partition 109, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 108.0 in stage 1.0 (TID 108) in 13 ms on localhost (executor driver) (108/200)
20/04/25 21:53:18 INFO Executor: Running task 109.0 in stage 1.0 (TID 109)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000109_109
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000109_109
20/04/25 21:53:18 INFO Executor: Finished task 109.0 in stage 1.0 (TID 109). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 110.0 in stage 1.0 (TID 110, localhost, executor driver, partition 110, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 110.0 in stage 1.0 (TID 110)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 109.0 in stage 1.0 (TID 109) in 13 ms on localhost (executor driver) (109/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000110_110
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000110_110
20/04/25 21:53:18 INFO Executor: Finished task 110.0 in stage 1.0 (TID 110). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 111.0 in stage 1.0 (TID 111, localhost, executor driver, partition 111, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 111.0 in stage 1.0 (TID 111)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 110.0 in stage 1.0 (TID 110) in 14 ms on localhost (executor driver) (110/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000111_111
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000111_111
20/04/25 21:53:18 INFO Executor: Finished task 111.0 in stage 1.0 (TID 111). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 112.0 in stage 1.0 (TID 112, localhost, executor driver, partition 112, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 111.0 in stage 1.0 (TID 111) in 14 ms on localhost (executor driver) (111/200)
20/04/25 21:53:18 INFO Executor: Running task 112.0 in stage 1.0 (TID 112)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000112_112
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000112_112
20/04/25 21:53:18 INFO Executor: Finished task 112.0 in stage 1.0 (TID 112). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 113.0 in stage 1.0 (TID 113, localhost, executor driver, partition 113, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 112.0 in stage 1.0 (TID 112) in 13 ms on localhost (executor driver) (112/200)
20/04/25 21:53:18 INFO Executor: Running task 113.0 in stage 1.0 (TID 113)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000113_113
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000113_113
20/04/25 21:53:18 INFO Executor: Finished task 113.0 in stage 1.0 (TID 113). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 114.0 in stage 1.0 (TID 114, localhost, executor driver, partition 114, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 114.0 in stage 1.0 (TID 114)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 113.0 in stage 1.0 (TID 113) in 13 ms on localhost (executor driver) (113/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000114_114
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000114_114
20/04/25 21:53:18 INFO Executor: Finished task 114.0 in stage 1.0 (TID 114). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 115.0 in stage 1.0 (TID 115, localhost, executor driver, partition 115, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 114.0 in stage 1.0 (TID 114) in 13 ms on localhost (executor driver) (114/200)
20/04/25 21:53:18 INFO Executor: Running task 115.0 in stage 1.0 (TID 115)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000115_115
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000115_115
20/04/25 21:53:18 INFO Executor: Finished task 115.0 in stage 1.0 (TID 115). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 116.0 in stage 1.0 (TID 116, localhost, executor driver, partition 116, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 116.0 in stage 1.0 (TID 116)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 115.0 in stage 1.0 (TID 115) in 14 ms on localhost (executor driver) (115/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000116_116
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000116_116
20/04/25 21:53:18 INFO Executor: Finished task 116.0 in stage 1.0 (TID 116). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 117.0 in stage 1.0 (TID 117, localhost, executor driver, partition 117, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 116.0 in stage 1.0 (TID 116) in 13 ms on localhost (executor driver) (116/200)
20/04/25 21:53:18 INFO Executor: Running task 117.0 in stage 1.0 (TID 117)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000117_117
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000117_117
20/04/25 21:53:18 INFO Executor: Finished task 117.0 in stage 1.0 (TID 117). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 118.0 in stage 1.0 (TID 118, localhost, executor driver, partition 118, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 118.0 in stage 1.0 (TID 118)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 117.0 in stage 1.0 (TID 117) in 15 ms on localhost (executor driver) (117/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000118_118
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000118_118
20/04/25 21:53:18 INFO Executor: Finished task 118.0 in stage 1.0 (TID 118). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 119.0 in stage 1.0 (TID 119, localhost, executor driver, partition 119, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 118.0 in stage 1.0 (TID 118) in 12 ms on localhost (executor driver) (118/200)
20/04/25 21:53:18 INFO Executor: Running task 119.0 in stage 1.0 (TID 119)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000119_119
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000119_119
20/04/25 21:53:18 INFO Executor: Finished task 119.0 in stage 1.0 (TID 119). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 120.0 in stage 1.0 (TID 120, localhost, executor driver, partition 120, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 120.0 in stage 1.0 (TID 120)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 119.0 in stage 1.0 (TID 119) in 13 ms on localhost (executor driver) (119/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000120_120
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000120_120
20/04/25 21:53:18 INFO Executor: Finished task 120.0 in stage 1.0 (TID 120). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 121.0 in stage 1.0 (TID 121, localhost, executor driver, partition 121, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 120.0 in stage 1.0 (TID 120) in 12 ms on localhost (executor driver) (120/200)
20/04/25 21:53:18 INFO Executor: Running task 121.0 in stage 1.0 (TID 121)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000121_121
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000121_121
20/04/25 21:53:18 INFO Executor: Finished task 121.0 in stage 1.0 (TID 121). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 122.0 in stage 1.0 (TID 122, localhost, executor driver, partition 122, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 122.0 in stage 1.0 (TID 122)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 121.0 in stage 1.0 (TID 121) in 16 ms on localhost (executor driver) (121/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000122_122
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000122_122
20/04/25 21:53:18 INFO Executor: Finished task 122.0 in stage 1.0 (TID 122). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 123.0 in stage 1.0 (TID 123, localhost, executor driver, partition 123, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 122.0 in stage 1.0 (TID 122) in 14 ms on localhost (executor driver) (122/200)
20/04/25 21:53:18 INFO Executor: Running task 123.0 in stage 1.0 (TID 123)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000123_123
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000123_123
20/04/25 21:53:18 INFO Executor: Finished task 123.0 in stage 1.0 (TID 123). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 124.0 in stage 1.0 (TID 124, localhost, executor driver, partition 124, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 124.0 in stage 1.0 (TID 124)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 123.0 in stage 1.0 (TID 123) in 16 ms on localhost (executor driver) (123/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000124_124
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000124_124
20/04/25 21:53:18 INFO Executor: Finished task 124.0 in stage 1.0 (TID 124). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 125.0 in stage 1.0 (TID 125, localhost, executor driver, partition 125, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 124.0 in stage 1.0 (TID 124) in 15 ms on localhost (executor driver) (124/200)
20/04/25 21:53:18 INFO Executor: Running task 125.0 in stage 1.0 (TID 125)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000125_125
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000125_125
20/04/25 21:53:18 INFO Executor: Finished task 125.0 in stage 1.0 (TID 125). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 126.0 in stage 1.0 (TID 126, localhost, executor driver, partition 126, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 126.0 in stage 1.0 (TID 126)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 125.0 in stage 1.0 (TID 125) in 19 ms on localhost (executor driver) (125/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000126_126
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000126_126
20/04/25 21:53:18 INFO Executor: Finished task 126.0 in stage 1.0 (TID 126). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 127.0 in stage 1.0 (TID 127, localhost, executor driver, partition 127, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 126.0 in stage 1.0 (TID 126) in 19 ms on localhost (executor driver) (126/200)
20/04/25 21:53:18 INFO Executor: Running task 127.0 in stage 1.0 (TID 127)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000127_127
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000127_127
20/04/25 21:53:18 INFO Executor: Finished task 127.0 in stage 1.0 (TID 127). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 128.0 in stage 1.0 (TID 128, localhost, executor driver, partition 128, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 128.0 in stage 1.0 (TID 128)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 127.0 in stage 1.0 (TID 127) in 23 ms on localhost (executor driver) (127/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000128_128
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000128_128
20/04/25 21:53:18 INFO Executor: Finished task 128.0 in stage 1.0 (TID 128). 2267 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 129.0 in stage 1.0 (TID 129, localhost, executor driver, partition 129, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 128.0 in stage 1.0 (TID 128) in 23 ms on localhost (executor driver) (128/200)
20/04/25 21:53:18 INFO Executor: Running task 129.0 in stage 1.0 (TID 129)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000129_129
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000129_129
20/04/25 21:53:18 INFO Executor: Finished task 129.0 in stage 1.0 (TID 129). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 131.0 in stage 1.0 (TID 130, localhost, executor driver, partition 131, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 131.0 in stage 1.0 (TID 130)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 129.0 in stage 1.0 (TID 129) in 14 ms on localhost (executor driver) (129/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000131_130
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000131_130
20/04/25 21:53:18 INFO Executor: Finished task 131.0 in stage 1.0 (TID 130). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 132.0 in stage 1.0 (TID 131, localhost, executor driver, partition 132, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 132.0 in stage 1.0 (TID 131)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 131.0 in stage 1.0 (TID 130) in 13 ms on localhost (executor driver) (130/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000132_131
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000132_131
20/04/25 21:53:18 INFO Executor: Finished task 132.0 in stage 1.0 (TID 131). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 133.0 in stage 1.0 (TID 132, localhost, executor driver, partition 133, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 133.0 in stage 1.0 (TID 132)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 132.0 in stage 1.0 (TID 131) in 13 ms on localhost (executor driver) (131/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000133_132
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000133_132
20/04/25 21:53:18 INFO Executor: Finished task 133.0 in stage 1.0 (TID 132). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 134.0 in stage 1.0 (TID 133, localhost, executor driver, partition 134, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 134.0 in stage 1.0 (TID 133)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 133.0 in stage 1.0 (TID 132) in 13 ms on localhost (executor driver) (132/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000134_133
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000134_133
20/04/25 21:53:18 INFO Executor: Finished task 134.0 in stage 1.0 (TID 133). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 135.0 in stage 1.0 (TID 134, localhost, executor driver, partition 135, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 135.0 in stage 1.0 (TID 134)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 134.0 in stage 1.0 (TID 133) in 14 ms on localhost (executor driver) (133/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000135_134
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000135_134
20/04/25 21:53:18 INFO Executor: Finished task 135.0 in stage 1.0 (TID 134). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 136.0 in stage 1.0 (TID 135, localhost, executor driver, partition 136, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 136.0 in stage 1.0 (TID 135)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 135.0 in stage 1.0 (TID 134) in 12 ms on localhost (executor driver) (134/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000136_135
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000136_135
20/04/25 21:53:18 INFO Executor: Finished task 136.0 in stage 1.0 (TID 135). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 137.0 in stage 1.0 (TID 136, localhost, executor driver, partition 137, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 137.0 in stage 1.0 (TID 136)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 136.0 in stage 1.0 (TID 135) in 12 ms on localhost (executor driver) (135/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000137_136
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000137_136
20/04/25 21:53:18 INFO Executor: Finished task 137.0 in stage 1.0 (TID 136). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 138.0 in stage 1.0 (TID 137, localhost, executor driver, partition 138, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 138.0 in stage 1.0 (TID 137)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 137.0 in stage 1.0 (TID 136) in 13 ms on localhost (executor driver) (136/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000138_137
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000138_137
20/04/25 21:53:18 INFO Executor: Finished task 138.0 in stage 1.0 (TID 137). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 139.0 in stage 1.0 (TID 138, localhost, executor driver, partition 139, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 139.0 in stage 1.0 (TID 138)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 138.0 in stage 1.0 (TID 137) in 12 ms on localhost (executor driver) (137/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000139_138
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000139_138
20/04/25 21:53:18 INFO Executor: Finished task 139.0 in stage 1.0 (TID 138). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 140.0 in stage 1.0 (TID 139, localhost, executor driver, partition 140, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 140.0 in stage 1.0 (TID 139)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 139.0 in stage 1.0 (TID 138) in 12 ms on localhost (executor driver) (138/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000140_139
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000140_139
20/04/25 21:53:18 INFO Executor: Finished task 140.0 in stage 1.0 (TID 139). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 141.0 in stage 1.0 (TID 140, localhost, executor driver, partition 141, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 141.0 in stage 1.0 (TID 140)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 140.0 in stage 1.0 (TID 139) in 11 ms on localhost (executor driver) (139/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000141_140
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000141_140
20/04/25 21:53:18 INFO Executor: Finished task 141.0 in stage 1.0 (TID 140). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 142.0 in stage 1.0 (TID 141, localhost, executor driver, partition 142, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 141.0 in stage 1.0 (TID 140) in 12 ms on localhost (executor driver) (140/200)
20/04/25 21:53:18 INFO Executor: Running task 142.0 in stage 1.0 (TID 141)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000142_141
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000142_141
20/04/25 21:53:18 INFO Executor: Finished task 142.0 in stage 1.0 (TID 141). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 143.0 in stage 1.0 (TID 142, localhost, executor driver, partition 143, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 143.0 in stage 1.0 (TID 142)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 142.0 in stage 1.0 (TID 141) in 11 ms on localhost (executor driver) (141/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000143_142
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000143_142
20/04/25 21:53:18 INFO Executor: Finished task 143.0 in stage 1.0 (TID 142). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 144.0 in stage 1.0 (TID 143, localhost, executor driver, partition 144, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 144.0 in stage 1.0 (TID 143)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 143.0 in stage 1.0 (TID 142) in 11 ms on localhost (executor driver) (142/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000144_143
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000144_143
20/04/25 21:53:18 INFO Executor: Finished task 144.0 in stage 1.0 (TID 143). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 145.0 in stage 1.0 (TID 144, localhost, executor driver, partition 145, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 145.0 in stage 1.0 (TID 144)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 144.0 in stage 1.0 (TID 143) in 12 ms on localhost (executor driver) (143/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000145_144
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000145_144
20/04/25 21:53:18 INFO Executor: Finished task 145.0 in stage 1.0 (TID 144). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 146.0 in stage 1.0 (TID 145, localhost, executor driver, partition 146, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 145.0 in stage 1.0 (TID 144) in 12 ms on localhost (executor driver) (144/200)
20/04/25 21:53:18 INFO Executor: Running task 146.0 in stage 1.0 (TID 145)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000146_145
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000146_145
20/04/25 21:53:18 INFO Executor: Finished task 146.0 in stage 1.0 (TID 145). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 147.0 in stage 1.0 (TID 146, localhost, executor driver, partition 147, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 147.0 in stage 1.0 (TID 146)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 146.0 in stage 1.0 (TID 145) in 11 ms on localhost (executor driver) (145/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000147_146
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000147_146
20/04/25 21:53:18 INFO Executor: Finished task 147.0 in stage 1.0 (TID 146). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 148.0 in stage 1.0 (TID 147, localhost, executor driver, partition 148, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 148.0 in stage 1.0 (TID 147)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 147.0 in stage 1.0 (TID 146) in 12 ms on localhost (executor driver) (146/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000148_147
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000148_147
20/04/25 21:53:18 INFO Executor: Finished task 148.0 in stage 1.0 (TID 147). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 149.0 in stage 1.0 (TID 148, localhost, executor driver, partition 149, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 148.0 in stage 1.0 (TID 147) in 11 ms on localhost (executor driver) (147/200)
20/04/25 21:53:18 INFO Executor: Running task 149.0 in stage 1.0 (TID 148)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000149_148
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000149_148
20/04/25 21:53:18 INFO Executor: Finished task 149.0 in stage 1.0 (TID 148). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 150.0 in stage 1.0 (TID 149, localhost, executor driver, partition 150, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 150.0 in stage 1.0 (TID 149)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 149.0 in stage 1.0 (TID 148) in 12 ms on localhost (executor driver) (148/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000150_149
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000150_149
20/04/25 21:53:18 INFO Executor: Finished task 150.0 in stage 1.0 (TID 149). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 151.0 in stage 1.0 (TID 150, localhost, executor driver, partition 151, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 151.0 in stage 1.0 (TID 150)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 150.0 in stage 1.0 (TID 149) in 12 ms on localhost (executor driver) (149/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000151_150
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000151_150
20/04/25 21:53:18 INFO Executor: Finished task 151.0 in stage 1.0 (TID 150). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 152.0 in stage 1.0 (TID 151, localhost, executor driver, partition 152, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 151.0 in stage 1.0 (TID 150) in 13 ms on localhost (executor driver) (150/200)
20/04/25 21:53:18 INFO Executor: Running task 152.0 in stage 1.0 (TID 151)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000152_151
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000152_151
20/04/25 21:53:18 INFO Executor: Finished task 152.0 in stage 1.0 (TID 151). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 153.0 in stage 1.0 (TID 152, localhost, executor driver, partition 153, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 153.0 in stage 1.0 (TID 152)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 152.0 in stage 1.0 (TID 151) in 16 ms on localhost (executor driver) (151/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000153_152
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000153_152
20/04/25 21:53:18 INFO Executor: Finished task 153.0 in stage 1.0 (TID 152). 2267 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 154.0 in stage 1.0 (TID 153, localhost, executor driver, partition 154, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 153.0 in stage 1.0 (TID 152) in 22 ms on localhost (executor driver) (152/200)
20/04/25 21:53:18 INFO Executor: Running task 154.0 in stage 1.0 (TID 153)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000154_153
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000154_153
20/04/25 21:53:18 INFO Executor: Finished task 154.0 in stage 1.0 (TID 153). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 155.0 in stage 1.0 (TID 154, localhost, executor driver, partition 155, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 155.0 in stage 1.0 (TID 154)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 154.0 in stage 1.0 (TID 153) in 14 ms on localhost (executor driver) (153/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000155_154
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000155_154
20/04/25 21:53:18 INFO Executor: Finished task 155.0 in stage 1.0 (TID 154). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 156.0 in stage 1.0 (TID 155, localhost, executor driver, partition 156, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 155.0 in stage 1.0 (TID 154) in 11 ms on localhost (executor driver) (154/200)
20/04/25 21:53:18 INFO Executor: Running task 156.0 in stage 1.0 (TID 155)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000156_155
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000156_155
20/04/25 21:53:18 INFO Executor: Finished task 156.0 in stage 1.0 (TID 155). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 157.0 in stage 1.0 (TID 156, localhost, executor driver, partition 157, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 157.0 in stage 1.0 (TID 156)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 156.0 in stage 1.0 (TID 155) in 12 ms on localhost (executor driver) (155/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000157_156
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000157_156
20/04/25 21:53:18 INFO Executor: Finished task 157.0 in stage 1.0 (TID 156). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 158.0 in stage 1.0 (TID 157, localhost, executor driver, partition 158, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 158.0 in stage 1.0 (TID 157)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 157.0 in stage 1.0 (TID 156) in 12 ms on localhost (executor driver) (156/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000158_157
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000158_157
20/04/25 21:53:18 INFO Executor: Finished task 158.0 in stage 1.0 (TID 157). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 159.0 in stage 1.0 (TID 158, localhost, executor driver, partition 159, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 158.0 in stage 1.0 (TID 157) in 12 ms on localhost (executor driver) (157/200)
20/04/25 21:53:18 INFO Executor: Running task 159.0 in stage 1.0 (TID 158)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000159_158
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000159_158
20/04/25 21:53:18 INFO Executor: Finished task 159.0 in stage 1.0 (TID 158). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 160.0 in stage 1.0 (TID 159, localhost, executor driver, partition 160, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 160.0 in stage 1.0 (TID 159)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 159.0 in stage 1.0 (TID 158) in 11 ms on localhost (executor driver) (158/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000160_159
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000160_159
20/04/25 21:53:18 INFO Executor: Finished task 160.0 in stage 1.0 (TID 159). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 161.0 in stage 1.0 (TID 160, localhost, executor driver, partition 161, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 160.0 in stage 1.0 (TID 159) in 11 ms on localhost (executor driver) (159/200)
20/04/25 21:53:18 INFO Executor: Running task 161.0 in stage 1.0 (TID 160)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000161_160
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000161_160
20/04/25 21:53:18 INFO Executor: Finished task 161.0 in stage 1.0 (TID 160). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 162.0 in stage 1.0 (TID 161, localhost, executor driver, partition 162, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 162.0 in stage 1.0 (TID 161)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 161.0 in stage 1.0 (TID 160) in 12 ms on localhost (executor driver) (160/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000162_161
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000162_161
20/04/25 21:53:18 INFO Executor: Finished task 162.0 in stage 1.0 (TID 161). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 163.0 in stage 1.0 (TID 162, localhost, executor driver, partition 163, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 163.0 in stage 1.0 (TID 162)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 162.0 in stage 1.0 (TID 161) in 12 ms on localhost (executor driver) (161/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000163_162
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000163_162
20/04/25 21:53:18 INFO Executor: Finished task 163.0 in stage 1.0 (TID 162). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 164.0 in stage 1.0 (TID 163, localhost, executor driver, partition 164, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 164.0 in stage 1.0 (TID 163)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 163.0 in stage 1.0 (TID 162) in 11 ms on localhost (executor driver) (162/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000164_163
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000164_163
20/04/25 21:53:18 INFO Executor: Finished task 164.0 in stage 1.0 (TID 163). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 165.0 in stage 1.0 (TID 164, localhost, executor driver, partition 165, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 165.0 in stage 1.0 (TID 164)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 164.0 in stage 1.0 (TID 163) in 11 ms on localhost (executor driver) (163/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000165_164
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000165_164
20/04/25 21:53:18 INFO Executor: Finished task 165.0 in stage 1.0 (TID 164). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 166.0 in stage 1.0 (TID 165, localhost, executor driver, partition 166, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 166.0 in stage 1.0 (TID 165)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 165.0 in stage 1.0 (TID 164) in 11 ms on localhost (executor driver) (164/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000166_165
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000166_165
20/04/25 21:53:18 INFO Executor: Finished task 166.0 in stage 1.0 (TID 165). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 167.0 in stage 1.0 (TID 166, localhost, executor driver, partition 167, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 166.0 in stage 1.0 (TID 165) in 12 ms on localhost (executor driver) (165/200)
20/04/25 21:53:18 INFO Executor: Running task 167.0 in stage 1.0 (TID 166)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000167_166
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000167_166
20/04/25 21:53:18 INFO Executor: Finished task 167.0 in stage 1.0 (TID 166). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 168.0 in stage 1.0 (TID 167, localhost, executor driver, partition 168, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 168.0 in stage 1.0 (TID 167)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 167.0 in stage 1.0 (TID 166) in 12 ms on localhost (executor driver) (166/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000168_167
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000168_167
20/04/25 21:53:18 INFO Executor: Finished task 168.0 in stage 1.0 (TID 167). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 169.0 in stage 1.0 (TID 168, localhost, executor driver, partition 169, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 169.0 in stage 1.0 (TID 168)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 168.0 in stage 1.0 (TID 167) in 12 ms on localhost (executor driver) (167/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000169_168
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000169_168
20/04/25 21:53:18 INFO Executor: Finished task 169.0 in stage 1.0 (TID 168). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 170.0 in stage 1.0 (TID 169, localhost, executor driver, partition 170, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 170.0 in stage 1.0 (TID 169)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 169.0 in stage 1.0 (TID 168) in 13 ms on localhost (executor driver) (168/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000170_169
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000170_169
20/04/25 21:53:18 INFO Executor: Finished task 170.0 in stage 1.0 (TID 169). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 171.0 in stage 1.0 (TID 170, localhost, executor driver, partition 171, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 171.0 in stage 1.0 (TID 170)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 170.0 in stage 1.0 (TID 169) in 12 ms on localhost (executor driver) (169/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000171_170
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000171_170
20/04/25 21:53:18 INFO Executor: Finished task 171.0 in stage 1.0 (TID 170). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 172.0 in stage 1.0 (TID 171, localhost, executor driver, partition 172, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 172.0 in stage 1.0 (TID 171)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 171.0 in stage 1.0 (TID 170) in 12 ms on localhost (executor driver) (170/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000172_171
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000172_171
20/04/25 21:53:18 INFO Executor: Finished task 172.0 in stage 1.0 (TID 171). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 173.0 in stage 1.0 (TID 172, localhost, executor driver, partition 173, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO Executor: Running task 173.0 in stage 1.0 (TID 172)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 172.0 in stage 1.0 (TID 171) in 11 ms on localhost (executor driver) (171/200)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000173_172
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000173_172
20/04/25 21:53:18 INFO Executor: Finished task 173.0 in stage 1.0 (TID 172). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 174.0 in stage 1.0 (TID 173, localhost, executor driver, partition 174, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 173.0 in stage 1.0 (TID 172) in 11 ms on localhost (executor driver) (172/200)
20/04/25 21:53:18 INFO Executor: Running task 174.0 in stage 1.0 (TID 173)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:18 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:18 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:18 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:18 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000174_173
20/04/25 21:53:18 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:18 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:18 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:18 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:18 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:18 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:18 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:18 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:18 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:18 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:18 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:18 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:18 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:18 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000174_173
20/04/25 21:53:18 INFO Executor: Finished task 174.0 in stage 1.0 (TID 173). 2224 bytes result sent to driver
20/04/25 21:53:18 INFO TaskSetManager: Starting task 175.0 in stage 1.0 (TID 174, localhost, executor driver, partition 175, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:18 INFO TaskSetManager: Finished task 174.0 in stage 1.0 (TID 173) in 12 ms on localhost (executor driver) (173/200)
20/04/25 21:53:18 INFO Executor: Running task 175.0 in stage 1.0 (TID 174)
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:19 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:19 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:19 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:19 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000175_174
20/04/25 21:53:19 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:19 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:19 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:19 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:19 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:19 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:19 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:19 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:19 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:19 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000175_174
20/04/25 21:53:19 INFO Executor: Finished task 175.0 in stage 1.0 (TID 174). 2224 bytes result sent to driver
20/04/25 21:53:19 INFO TaskSetManager: Starting task 176.0 in stage 1.0 (TID 175, localhost, executor driver, partition 176, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:19 INFO Executor: Running task 176.0 in stage 1.0 (TID 175)
20/04/25 21:53:19 INFO TaskSetManager: Finished task 175.0 in stage 1.0 (TID 174) in 12 ms on localhost (executor driver) (174/200)
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:19 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:19 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:19 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:19 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000176_175
20/04/25 21:53:19 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:19 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:19 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:19 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:19 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:19 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:19 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:19 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:19 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:19 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000176_175
20/04/25 21:53:19 INFO Executor: Finished task 176.0 in stage 1.0 (TID 175). 2267 bytes result sent to driver
20/04/25 21:53:19 INFO TaskSetManager: Starting task 177.0 in stage 1.0 (TID 176, localhost, executor driver, partition 177, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:19 INFO Executor: Running task 177.0 in stage 1.0 (TID 176)
20/04/25 21:53:19 INFO TaskSetManager: Finished task 176.0 in stage 1.0 (TID 175) in 12 ms on localhost (executor driver) (175/200)
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:19 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:19 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:19 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:19 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000177_176
20/04/25 21:53:19 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:19 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:19 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:19 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:19 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:19 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:19 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:19 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:19 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:19 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000177_176
20/04/25 21:53:19 INFO Executor: Finished task 177.0 in stage 1.0 (TID 176). 2224 bytes result sent to driver
20/04/25 21:53:19 INFO TaskSetManager: Starting task 178.0 in stage 1.0 (TID 177, localhost, executor driver, partition 178, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:19 INFO Executor: Running task 178.0 in stage 1.0 (TID 177)
20/04/25 21:53:19 INFO TaskSetManager: Finished task 177.0 in stage 1.0 (TID 176) in 17 ms on localhost (executor driver) (176/200)
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:19 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:19 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:19 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:19 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000178_177
20/04/25 21:53:19 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:19 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:19 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:19 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:19 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:19 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:19 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:19 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:19 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:19 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000178_177
20/04/25 21:53:19 INFO Executor: Finished task 178.0 in stage 1.0 (TID 177). 2224 bytes result sent to driver
20/04/25 21:53:19 INFO TaskSetManager: Starting task 179.0 in stage 1.0 (TID 178, localhost, executor driver, partition 179, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:19 INFO TaskSetManager: Finished task 178.0 in stage 1.0 (TID 177) in 18 ms on localhost (executor driver) (177/200)
20/04/25 21:53:19 INFO Executor: Running task 179.0 in stage 1.0 (TID 178)
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:19 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:19 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:19 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:19 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000179_178
20/04/25 21:53:19 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:19 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:19 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:19 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:19 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:19 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:19 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:19 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:19 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:19 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000179_178
20/04/25 21:53:19 INFO Executor: Finished task 179.0 in stage 1.0 (TID 178). 2224 bytes result sent to driver
20/04/25 21:53:19 INFO TaskSetManager: Starting task 180.0 in stage 1.0 (TID 179, localhost, executor driver, partition 180, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:19 INFO Executor: Running task 180.0 in stage 1.0 (TID 179)
20/04/25 21:53:19 INFO TaskSetManager: Finished task 179.0 in stage 1.0 (TID 178) in 19 ms on localhost (executor driver) (178/200)
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:19 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:19 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:19 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:19 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000180_179
20/04/25 21:53:19 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:19 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:19 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:19 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:19 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:19 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:19 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:19 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:19 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:19 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000180_179
20/04/25 21:53:19 INFO Executor: Finished task 180.0 in stage 1.0 (TID 179). 2224 bytes result sent to driver
20/04/25 21:53:19 INFO TaskSetManager: Starting task 181.0 in stage 1.0 (TID 180, localhost, executor driver, partition 181, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:19 INFO Executor: Running task 181.0 in stage 1.0 (TID 180)
20/04/25 21:53:19 INFO TaskSetManager: Finished task 180.0 in stage 1.0 (TID 179) in 23 ms on localhost (executor driver) (179/200)
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:19 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:19 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:19 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:19 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000181_180
20/04/25 21:53:19 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:19 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:19 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:19 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:19 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:19 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:19 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:19 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:19 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:19 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000181_180
20/04/25 21:53:19 INFO Executor: Finished task 181.0 in stage 1.0 (TID 180). 2267 bytes result sent to driver
20/04/25 21:53:19 INFO TaskSetManager: Starting task 182.0 in stage 1.0 (TID 181, localhost, executor driver, partition 182, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:19 INFO TaskSetManager: Finished task 181.0 in stage 1.0 (TID 180) in 23 ms on localhost (executor driver) (180/200)
20/04/25 21:53:19 INFO Executor: Running task 182.0 in stage 1.0 (TID 181)
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:19 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:19 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:19 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:19 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000182_181
20/04/25 21:53:19 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:19 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:19 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:19 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:19 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:19 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:19 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:19 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:19 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:19 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000182_181
20/04/25 21:53:19 INFO Executor: Finished task 182.0 in stage 1.0 (TID 181). 2224 bytes result sent to driver
20/04/25 21:53:19 INFO TaskSetManager: Starting task 183.0 in stage 1.0 (TID 182, localhost, executor driver, partition 183, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:19 INFO TaskSetManager: Finished task 182.0 in stage 1.0 (TID 181) in 12 ms on localhost (executor driver) (181/200)
20/04/25 21:53:19 INFO Executor: Running task 183.0 in stage 1.0 (TID 182)
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:19 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:19 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:19 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:19 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000183_182
20/04/25 21:53:19 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:19 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:19 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:19 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:19 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:19 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:19 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:19 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:19 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:19 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000183_182
20/04/25 21:53:19 INFO Executor: Finished task 183.0 in stage 1.0 (TID 182). 2224 bytes result sent to driver
20/04/25 21:53:19 INFO TaskSetManager: Starting task 184.0 in stage 1.0 (TID 183, localhost, executor driver, partition 184, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:19 INFO Executor: Running task 184.0 in stage 1.0 (TID 183)
20/04/25 21:53:19 INFO TaskSetManager: Finished task 183.0 in stage 1.0 (TID 182) in 12 ms on localhost (executor driver) (182/200)
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:19 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:19 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:19 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:19 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000184_183
20/04/25 21:53:19 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:19 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:19 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:19 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:19 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:19 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:19 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:19 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:19 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:19 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000184_183
20/04/25 21:53:19 INFO Executor: Finished task 184.0 in stage 1.0 (TID 183). 2224 bytes result sent to driver
20/04/25 21:53:19 INFO TaskSetManager: Starting task 185.0 in stage 1.0 (TID 184, localhost, executor driver, partition 185, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:19 INFO Executor: Running task 185.0 in stage 1.0 (TID 184)
20/04/25 21:53:19 INFO TaskSetManager: Finished task 184.0 in stage 1.0 (TID 183) in 12 ms on localhost (executor driver) (183/200)
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:19 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:19 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:19 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:19 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000185_184
20/04/25 21:53:19 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:19 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:19 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:19 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:19 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:19 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:19 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:19 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:19 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:19 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000185_184
20/04/25 21:53:19 INFO Executor: Finished task 185.0 in stage 1.0 (TID 184). 2224 bytes result sent to driver
20/04/25 21:53:19 INFO TaskSetManager: Starting task 186.0 in stage 1.0 (TID 185, localhost, executor driver, partition 186, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:19 INFO Executor: Running task 186.0 in stage 1.0 (TID 185)
20/04/25 21:53:19 INFO TaskSetManager: Finished task 185.0 in stage 1.0 (TID 184) in 12 ms on localhost (executor driver) (184/200)
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:19 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:19 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:19 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:19 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000186_185
20/04/25 21:53:19 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:19 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:19 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:19 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:19 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:19 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:19 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:19 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:19 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:19 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000186_185
20/04/25 21:53:19 INFO Executor: Finished task 186.0 in stage 1.0 (TID 185). 2224 bytes result sent to driver
20/04/25 21:53:19 INFO TaskSetManager: Starting task 187.0 in stage 1.0 (TID 186, localhost, executor driver, partition 187, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:19 INFO TaskSetManager: Finished task 186.0 in stage 1.0 (TID 185) in 12 ms on localhost (executor driver) (185/200)
20/04/25 21:53:19 INFO Executor: Running task 187.0 in stage 1.0 (TID 186)
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:19 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:19 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:19 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:19 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000187_186
20/04/25 21:53:19 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:19 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:19 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:19 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:19 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:19 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:19 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:19 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:19 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:19 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000187_186
20/04/25 21:53:19 INFO Executor: Finished task 187.0 in stage 1.0 (TID 186). 2224 bytes result sent to driver
20/04/25 21:53:19 INFO TaskSetManager: Starting task 188.0 in stage 1.0 (TID 187, localhost, executor driver, partition 188, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:19 INFO Executor: Running task 188.0 in stage 1.0 (TID 187)
20/04/25 21:53:19 INFO TaskSetManager: Finished task 187.0 in stage 1.0 (TID 186) in 11 ms on localhost (executor driver) (186/200)
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:19 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:19 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:19 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:19 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000188_187
20/04/25 21:53:19 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:19 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:19 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:19 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:19 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:19 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:19 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:19 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:19 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:19 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000188_187
20/04/25 21:53:19 INFO Executor: Finished task 188.0 in stage 1.0 (TID 187). 2224 bytes result sent to driver
20/04/25 21:53:19 INFO TaskSetManager: Starting task 189.0 in stage 1.0 (TID 188, localhost, executor driver, partition 189, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:19 INFO TaskSetManager: Finished task 188.0 in stage 1.0 (TID 187) in 11 ms on localhost (executor driver) (187/200)
20/04/25 21:53:19 INFO Executor: Running task 189.0 in stage 1.0 (TID 188)
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:19 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:19 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:19 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:19 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000189_188
20/04/25 21:53:19 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:19 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:19 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:19 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:19 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:19 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:19 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:19 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:19 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:19 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000189_188
20/04/25 21:53:19 INFO Executor: Finished task 189.0 in stage 1.0 (TID 188). 2224 bytes result sent to driver
20/04/25 21:53:19 INFO TaskSetManager: Starting task 190.0 in stage 1.0 (TID 189, localhost, executor driver, partition 190, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:19 INFO Executor: Running task 190.0 in stage 1.0 (TID 189)
20/04/25 21:53:19 INFO TaskSetManager: Finished task 189.0 in stage 1.0 (TID 188) in 12 ms on localhost (executor driver) (188/200)
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:19 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:19 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:19 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:19 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000190_189
20/04/25 21:53:19 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:19 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:19 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:19 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:19 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:19 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:19 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:19 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:19 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:19 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000190_189
20/04/25 21:53:19 INFO Executor: Finished task 190.0 in stage 1.0 (TID 189). 2224 bytes result sent to driver
20/04/25 21:53:19 INFO TaskSetManager: Starting task 191.0 in stage 1.0 (TID 190, localhost, executor driver, partition 191, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:19 INFO Executor: Running task 191.0 in stage 1.0 (TID 190)
20/04/25 21:53:19 INFO TaskSetManager: Finished task 190.0 in stage 1.0 (TID 189) in 12 ms on localhost (executor driver) (189/200)
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:19 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:19 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:19 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:19 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000191_190
20/04/25 21:53:19 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:19 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:19 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:19 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:19 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:19 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:19 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:19 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:19 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:19 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000191_190
20/04/25 21:53:19 INFO Executor: Finished task 191.0 in stage 1.0 (TID 190). 2224 bytes result sent to driver
20/04/25 21:53:19 INFO TaskSetManager: Starting task 192.0 in stage 1.0 (TID 191, localhost, executor driver, partition 192, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:19 INFO TaskSetManager: Finished task 191.0 in stage 1.0 (TID 190) in 13 ms on localhost (executor driver) (190/200)
20/04/25 21:53:19 INFO Executor: Running task 192.0 in stage 1.0 (TID 191)
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:19 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:19 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:19 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:19 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000192_191
20/04/25 21:53:19 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:19 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:19 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:19 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:19 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:19 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:19 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:19 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:19 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:19 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000192_191
20/04/25 21:53:19 INFO Executor: Finished task 192.0 in stage 1.0 (TID 191). 2224 bytes result sent to driver
20/04/25 21:53:19 INFO TaskSetManager: Starting task 193.0 in stage 1.0 (TID 192, localhost, executor driver, partition 193, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:19 INFO Executor: Running task 193.0 in stage 1.0 (TID 192)
20/04/25 21:53:19 INFO TaskSetManager: Finished task 192.0 in stage 1.0 (TID 191) in 14 ms on localhost (executor driver) (191/200)
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/04/25 21:53:19 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:19 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:19 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:19 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000193_192
20/04/25 21:53:19 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:19 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:19 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:19 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:19 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:19 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:19 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:19 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:19 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:19 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000193_192
20/04/25 21:53:19 INFO Executor: Finished task 193.0 in stage 1.0 (TID 192). 2224 bytes result sent to driver
20/04/25 21:53:19 INFO TaskSetManager: Starting task 194.0 in stage 1.0 (TID 193, localhost, executor driver, partition 194, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:19 INFO Executor: Running task 194.0 in stage 1.0 (TID 193)
20/04/25 21:53:19 INFO TaskSetManager: Finished task 193.0 in stage 1.0 (TID 192) in 14 ms on localhost (executor driver) (192/200)
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:19 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:19 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:19 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:19 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000194_193
20/04/25 21:53:19 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:19 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:19 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:19 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:19 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:19 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:19 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:19 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:19 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:19 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000194_193
20/04/25 21:53:19 INFO Executor: Finished task 194.0 in stage 1.0 (TID 193). 2224 bytes result sent to driver
20/04/25 21:53:19 INFO TaskSetManager: Starting task 195.0 in stage 1.0 (TID 194, localhost, executor driver, partition 195, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:19 INFO Executor: Running task 195.0 in stage 1.0 (TID 194)
20/04/25 21:53:19 INFO TaskSetManager: Finished task 194.0 in stage 1.0 (TID 193) in 13 ms on localhost (executor driver) (193/200)
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:19 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:19 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:19 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:19 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000195_194
20/04/25 21:53:19 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:19 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:19 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:19 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:19 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:19 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:19 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:19 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:19 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:19 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000195_194
20/04/25 21:53:19 INFO Executor: Finished task 195.0 in stage 1.0 (TID 194). 2224 bytes result sent to driver
20/04/25 21:53:19 INFO TaskSetManager: Starting task 196.0 in stage 1.0 (TID 195, localhost, executor driver, partition 196, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:19 INFO TaskSetManager: Finished task 195.0 in stage 1.0 (TID 194) in 13 ms on localhost (executor driver) (194/200)
20/04/25 21:53:19 INFO Executor: Running task 196.0 in stage 1.0 (TID 195)
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/04/25 21:53:19 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:19 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:19 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:19 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000196_195
20/04/25 21:53:19 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:19 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:19 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:19 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:19 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:19 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:19 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:19 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:19 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:19 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000196_195
20/04/25 21:53:19 INFO Executor: Finished task 196.0 in stage 1.0 (TID 195). 2224 bytes result sent to driver
20/04/25 21:53:19 INFO TaskSetManager: Starting task 197.0 in stage 1.0 (TID 196, localhost, executor driver, partition 197, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:19 INFO TaskSetManager: Finished task 196.0 in stage 1.0 (TID 195) in 14 ms on localhost (executor driver) (195/200)
20/04/25 21:53:19 INFO Executor: Running task 197.0 in stage 1.0 (TID 196)
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/04/25 21:53:19 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:19 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:19 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:19 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000197_196
20/04/25 21:53:19 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:19 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:19 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:19 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:19 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:19 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:19 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:19 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:19 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:19 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000197_196
20/04/25 21:53:19 INFO Executor: Finished task 197.0 in stage 1.0 (TID 196). 2224 bytes result sent to driver
20/04/25 21:53:19 INFO TaskSetManager: Starting task 198.0 in stage 1.0 (TID 197, localhost, executor driver, partition 198, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:19 INFO Executor: Running task 198.0 in stage 1.0 (TID 197)
20/04/25 21:53:19 INFO TaskSetManager: Finished task 197.0 in stage 1.0 (TID 196) in 13 ms on localhost (executor driver) (196/200)
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:19 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:19 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:19 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:19 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000198_197
20/04/25 21:53:19 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:19 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:19 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:19 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:19 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:19 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:19 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:19 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:19 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:19 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000198_197
20/04/25 21:53:19 INFO Executor: Finished task 198.0 in stage 1.0 (TID 197). 2224 bytes result sent to driver
20/04/25 21:53:19 INFO TaskSetManager: Starting task 199.0 in stage 1.0 (TID 198, localhost, executor driver, partition 199, PROCESS_LOCAL, 4726 bytes)
20/04/25 21:53:19 INFO TaskSetManager: Finished task 198.0 in stage 1.0 (TID 197) in 16 ms on localhost (executor driver) (197/200)
20/04/25 21:53:19 INFO Executor: Running task 199.0 in stage 1.0 (TID 198)
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 1 blocks
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:19 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:19 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:19 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:19 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000199_198
20/04/25 21:53:19 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:19 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:19 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:19 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:19 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:19 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:19 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:19 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:19 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:19 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: 0 total items written
20/04/25 21:53:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000199_198
20/04/25 21:53:19 INFO Executor: Finished task 199.0 in stage 1.0 (TID 198). 2224 bytes result sent to driver
20/04/25 21:53:19 INFO TaskSetManager: Starting task 67.0 in stage 1.0 (TID 199, localhost, executor driver, partition 67, ANY, 4726 bytes)
20/04/25 21:53:19 INFO Executor: Running task 67.0 in stage 1.0 (TID 199)
20/04/25 21:53:19 INFO TaskSetManager: Finished task 199.0 in stage 1.0 (TID 198) in 15 ms on localhost (executor driver) (198/200)
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:19 INFO CodeGenerator: Code generated in 7.196748 ms
20/04/25 21:53:19 INFO CodeGenerator: Code generated in 6.735239 ms
20/04/25 21:53:19 INFO CodeGenerator: Code generated in 5.703121 ms
20/04/25 21:53:19 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:19 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:19 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:19 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000067_199
20/04/25 21:53:19 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:19 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:19 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:19 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:19 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:19 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:19 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:19 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:19 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:19 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 0,ItemCount: 0,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: 1 total items written
20/04/25 21:53:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000067_199
20/04/25 21:53:19 INFO Executor: Finished task 67.0 in stage 1.0 (TID 199). 2267 bytes result sent to driver
20/04/25 21:53:19 INFO TaskSetManager: Starting task 130.0 in stage 1.0 (TID 200, localhost, executor driver, partition 130, ANY, 4726 bytes)
20/04/25 21:53:19 INFO Executor: Running task 130.0 in stage 1.0 (TID 200)
20/04/25 21:53:19 INFO TaskSetManager: Finished task 67.0 in stage 1.0 (TID 199) in 72 ms on localhost (executor driver) (199/200)
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
20/04/25 21:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/04/25 21:53:19 INFO DynamoDBUtil: Using endpoint for DynamoDB: http://localhost:8000/
20/04/25 21:53:19 INFO JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
20/04/25 21:53:19 INFO WriteIopsCalculator: Table name: SharePeakPriceMonth
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput percent: 0.5
20/04/25 21:53:19 INFO WriteIopsCalculator: Task Id: attempt_20200425215315_0001_m_000130_200
20/04/25 21:53:19 INFO WriteIopsCalculator: Total map tasks: 1
20/04/25 21:53:19 INFO TaskCalculator: Cluster has 1 active nodes.
20/04/25 21:53:19 WARN ClusterTopologyNodeCapacityProvider: Exception when trying to determine instance types
java.nio.file.NoSuchFileException: /mnt/var/lib/info/job-flow.json
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.readJobFlowJsonString(ClusterTopologyNodeCapacityProvider.java:103)
	at org.apache.hadoop.dynamodb.util.ClusterTopologyNodeCapacityProvider.getCoreNodeMemoryMB(ClusterTopologyNodeCapacityProvider.java:42)
	at org.apache.hadoop.dynamodb.util.TaskCalculator.getMaxMapTasks(TaskCalculator.java:54)
	at org.apache.hadoop.dynamodb.DynamoDBUtil.calcMaxMapTasks(DynamoDBUtil.java:273)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.calculateMaxMapTasks(WriteIopsCalculator.java:78)
	at org.apache.hadoop.dynamodb.write.WriteIopsCalculator.<init>(WriteIopsCalculator.java:61)
	at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.<init>(AbstractDynamoDBRecordWriter.java:85)
	at org.apache.hadoop.dynamodb.write.DefaultDynamoDBRecordWriter.<init>(DefaultDynamoDBRecordWriter.java:27)
	at org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat.getRecordWriter(DynamoDBOutputFormat.java:30)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/25 21:53:19 INFO TaskCalculator: Slot size: 1024MB.
20/04/25 21:53:19 INFO TaskCalculator: Node manager can allocate 8192MB (8 slots) for containers on each node.
20/04/25 21:53:19 INFO TaskCalculator: Each mapper needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: Each reducer needs: 1024MB. (1 slots)
20/04/25 21:53:19 INFO TaskCalculator: MapReduce Application Manager needs: 1536 MB. (2 slots)
20/04/25 21:53:19 INFO TaskCalculator: Number of reducers: 1
20/04/25 21:53:19 INFO TaskCalculator: Max number of cluster map tasks: 5
20/04/25 21:53:19 INFO WriteIopsCalculator: Max parallel map tasks: 1
20/04/25 21:53:19 INFO DynamoDBClient: Describe table output: {Table: {AttributeDefinitions: [{AttributeName: stockShareSymbol,AttributeType: S}, {AttributeName: month,AttributeType: S}],TableName: SharePeakPriceMonth,KeySchema: [{AttributeName: stockShareSymbol,KeyType: HASH}, {AttributeName: month,KeyType: RANGE}],TableStatus: ACTIVE,CreationDateTime: Sat Apr 25 21:53:11 BRT 2020,ProvisionedThroughput: {LastIncreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,LastDecreaseDateTime: Wed Dec 31 21:00:00 BRT 1969,NumberOfDecreasesToday: 0,ReadCapacityUnits: 1000,WriteCapacityUnits: 1000},TableSizeBytes: 44,ItemCount: 1,TableArn: arn:aws:dynamodb:ddblocal:000000000000:table/SharePeakPriceMonth,}}
20/04/25 21:53:19 INFO WriteIopsCalculator: Throughput per task for table SharePeakPriceMonth : 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: Number of allocated item writes per second: 1
20/04/25 21:53:19 INFO AbstractDynamoDBRecordWriter: 1 total items written
20/04/25 21:53:19 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20200425215315_0001_m_000130_200
20/04/25 21:53:19 INFO Executor: Finished task 130.0 in stage 1.0 (TID 200). 2267 bytes result sent to driver
20/04/25 21:53:19 INFO TaskSetManager: Finished task 130.0 in stage 1.0 (TID 200) in 22 ms on localhost (executor driver) (200/200)
20/04/25 21:53:19 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/04/25 21:53:19 INFO DAGScheduler: ResultStage 1 (saveAsHadoopDataset at Main.java:250) finished in 3,477 s
20/04/25 21:53:19 INFO DAGScheduler: Job 0 finished: saveAsHadoopDataset at Main.java:250, took 3,902843 s
20/04/25 21:53:19 WARN FileOutputCommitter: Output Path is null in commitJob()
]]></system-err>
</testsuite>
